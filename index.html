<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JNN7F7HDFD"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-JNN7F7HDFD');
  </script>
  <meta name="description" content="AI-driven visual monitoring of industrial assembly tasks">
  <meta name="keywords" content="ViMAT, Visual Monitoring, Industrial Assembly, AI, Computer Vision, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AI-driven visual monitoring of industrial assembly tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bulma@1.0.2/css/bulma.min.css"
  >
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AI-driven visual monitoring of industrial assembly tasks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://tev.fbk.eu/team/mattia-nardon" target="_blank">Mattia Nardon</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://tev.fbk.eu/team/stefano-messelodi" target="_blank">Stefano Messelodi</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  Antonio Granata<sup>2</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://fabiopoiesi.github.io/" target="_blank">Fabio Poiesi</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  Alberto Danese<sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://davideboscaini.github.io/" target="_blank">Davide Boscaini</a><sup>1</sup>
                </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Fondazione Bruno Kessler,</span>
                    <span class="author-block"><sup>2</sup>Meccanica del Sarca s.p.a.</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                    <a href="https://arxiv.org/abs/2506.15285" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                      </a>
                    </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="Teaser Image" style="width: 100vw; height: auto; display: block;">
      <h2 class="subtitle" style="text-align: justify;">
          We present ViMAT, a novel system for the real-time visual monitoring of industrial assembly tasks. Given prior knowledge on assembly instructions (top left) and synthetic CAD models of assembly components (bottom left), ViMAT integrates an AI-driven perception module, which extracts visual observations from real-world video streams (top right), with a probabilistic reasoning module that predicts the assembly state from these observations (bottom center-right).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual monitoring of industrial assembly tasks is critical for preventing equipment damage due to procedural errors and ensuring worker safety.
            Although commercial solutions exist, they typically require rigid workspace setups or the application of visual markers to simplify the problem.
            We introduce ViMAT, a novel AI-driven system for real-time visual monitoring of assembly tasks that operates without these constraints.
            ViMAT combines a perception module that extracts visual observations from multi-view video streams with a reasoning module that infers the most likely action being performed based on the observed assembly state and prior task knowledge.
            We validate ViMAT on two assembly tasks, involving the replacement of LEGO components and the reconfiguration of hydraulic press molds, demonstrating its effectiveness through quantitative and qualitative analysis in challenging real-world scenarios characterized by partial and uncertain visual observations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview image-->
<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered" >ViMAT's overview</h2>
      <img src="./static/images/diagram.png" alt="Overview of ViMAT" style="width: 100vw; height: auto; display: block;">
      <h2 class="subtitle" style="text-align: justify;">
        Overview of ViMAT. Multi-view video frames are processed by the perception module (pink) to detect assembly components using a detector trained on a synthetic dataset generated by the digital twin module (green). These detections, along with prior task knowledge (assembly instructions), are passed to the probabilistic reasoning module (azure) to estimate the action being performed.
      </h2>
    </div>
  </div>
</section>
<!-- End Overview image -->


<!-- LEGO Scenario -->
<section class="hero teaser"></section>
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-top: 2rem; margin-bottom: 1rem;">LEGO Scenario</h2>
    <h2 class="subtitle" style="text-align: justify; margin-top: 2rem;"></h2>
      In this scenario, the assembly task consists of modifying a LEGO structure by adding or removing elements according to a predefined goal configuration. Components are picked from an input tray and placed onto the structure, while removed pieces are relocated to an output tray. A human operator performs the task, and the system continuously observes the scene through multiple calibrated RGBD sensors, identifying and tracking the components to estimate the most probable current assembly configuration.
    </h2>
    <div class="hero-body">
      <img src="./static/images/LEGEND.png" alt="LEGO Legend Image" style="width: 100vw; height: auto; display: block;">
      <h2 class="subtitle" style="text-align: justify;">
        Visualization of the digital LEGO components and the corresponding input/output trays utilized in the LEGO scenario task.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2> -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JUsE0Jo1hMY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <h2 class="subtitle" style="margin-top: 1rem; text-align: justify;">
            Demonstration of ViMAT in action on the LEGO assembly scenario. The video showcases real-time visual monitoring and reasoning: RGBD camera views with object detections (top and bottom-left), and a live bar chart (bottom-right) representing the system's estimated probabilities for each assembly state.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of LEGO Scenario -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{nardon2025vimat,
  title={AI-driven visual monitoring of industrial assembly tasks},
  author={Nardon, Mattia and Messelodi, Stefano and Granata, Antonio and Poiesi, Fabio and Danese, Alberto and Boscaini, Davide},
  booktitle={Proceedings of the International Conference on Image Analysis and Processing (ICIAP)},
  year={2025}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section class="section is light" id="acknowledgments">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      This work has been partially funded by the Provincia Autonoma di Trento (Italy) under L.P. 6/99, as part of the NEXTMAG project.
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <!-- Link to Paper -->
          <a class="icon-link" href="https://arxiv.org/pdf/2506.15285" target="_blank" title="View Paper on arXiv">
              <i class="fas fa-file-pdf fa-2x"></i>
          </a>
      </div>
      <div class="columns is-centered" style="margin-top: 20px;">
          <div class="column is-8">
              <div class="content has-text-centered">
                  <p>
                      This website is licensed under a
                      <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
                          Creative Commons Attribution-ShareAlike 4.0 International License
                      </a>.
                  </p>
                  <p>
                      Template adapted from
                      <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
                  </p>
              </div>
          </div>
      </div>
  </div>
</footer>


  </body>
  </html>
